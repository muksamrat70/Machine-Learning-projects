{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Costa Rican Household Poverty Level Prediction\n\nIn this notebook, get introduced to the problem, then perform a thorough Exploratory Data Analysis of the dataset, work on feature engineering, try out multiple machine learning models, optimize those models, inspect the outputs of the models, draw conclusions and finally select a model to predict. \n\nData Explanation\nThe data is provided in two files: train.csv and test.csv. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents one individual and each column is a feature, either unique to the individual, or for the household of the individual. The training set has one additional column, Target, which represents the poverty level on a 1-4 scale and are the labels for this dataset. A value of 1 is the most extreme poverty, 2 = moderate poverty, 3 = vulnerable households and 4 = non vulnerable households\n\nThis is a supervised multi-class classification machine learning problem\n\nObjective\nThe objective is to predict poverty on a household level. Moreover, we have to make a prediction for every individual in the test set, but \"ONLY the heads of household are used in scoring\" which means we want to predict poverty on a household basis.\nWhile all members of a household should have the same label in the training data, there are errors where individuals in the same household have different labels. In these cases, we are told to use the label for the head of each household, which can be identified by the rows where parentesco1 == 1.0. \n\nOf all 143 columns, few to note are below:\nidhogar: a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier.\nparentesco1: indicates if this person is the head of the household.\nTarget: the label, which should be equal for all members in a household\n\nWhile making the model, we'll train on a household basis with the label for each household the poverty level of the head of household. The raw data contains a mix of both household and individual characteristics and for the individual data, we will have to find a way to aggregate this for each household. Some of the individuals belong to a household with no head of household which means that unfortunately we can't use this data for training. \n\nRoadmap\nThe end objective is a machine learning model that can predict the poverty level of a household. We want to evaluate numerous models before choosing one as the \"best\" and after building a model, we want to investigate the predictions. The roadmap would be as follows :\n\n1. Understand the problem \n2. Exploratory Data Analysis\n3. Feature engineering to create a dataset for machine learning\n4. Compare several machine learning models\n5. Optimize all the models\n6. Investigate model predictions in context of problem\n7. Select the best model\n8. Draw conclusions and lay out next steps\n\nGetting Started\n\nImports - Data science libraries: Pandas, numpy, matplotlib, seaborn, and eventually sklearn for modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.1 Load pandas, numpy and matplotlib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import plot_importance\n\n# Image manipulation\nfrom skimage.io import imshow, imsave\n\n# Image normalizing and compression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\n\n# Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif  # Selection criteria\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n#for data processing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\n\n# for modeling estimators\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\n\n# for measuring performance\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\n#for tuning parameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nfrom eli5.sklearn import PermutationImportance\n\n# Misc.\nimport os\nimport time\nimport gc\nimport random\nfrom scipy.stats import uniform\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data and look at Summary Information"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 150\n\n# Read in data\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids=test['Id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', \n                                                                             figsize = (8, 6),\n                                                                            edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', \n                                                                             figsize = (8, 6),\n                                                                            edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {\"yes\": 1, \"no\": 0}\n\n# Apply same operation to both train and test\nfor df in [train, test]:\n    # Fill in the values with the correct mapping\n    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n\ntrain[['dependency', 'edjefa', 'edjefe']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['dependency', 'edjefa', 'edjefe']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'\n\nfrom collections import OrderedDict\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\nplt.figure(figsize = (16, 12))\nplt.style.use('fivethirtyeight')\n\n# Iterate through the float columns\nfor i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n    ax = plt.subplot(3, 1, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing in each column\nmissingte = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissingte['percent'] = missingte['total'] / len(test)\n\nmissingte.sort_values('percent', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables indicating home ownership\nown_variables = [x for x in train if x.startswith('tipo')]\n\n\n# Plot of the home ownership variables for home missing rent payments\ntrain.loc[train['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),\n                                                                        color = 'green',\n                                                              edgecolor = 'k', linewidth = 2);\nplt.xticks([0, 1, 2, 3, 4],\n           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n          rotation = 60)\nplt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables indicating home ownership\nown_variableste = [x for x in test if x.startswith('tipo')]\n\n\n# Plot of the home ownership variables for home missing rent payments\ntest.loc[test['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),\n                                                                        color = 'green',\n                                                              edgecolor = 'k', linewidth = 2);\nplt.xticks([0, 1, 2, 3, 4],\n           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n          rotation = 60)\nplt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in households that own the house with 0 rent payment\ntrain.loc[(train['tipovivi1'] == 1), 'v2a1'] = 0\n\n# Create missing rent payment column\ntrain['v2a1-missing'] = train['v2a1'].isnull()\n\ntrain['v2a1-missing'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in households that own the house with 0 rent payment\ntest.loc[(test['tipovivi1'] == 1), 'v2a1'] = 0\n\n# Create missing rent payment column\ntest['v2a1-missing'] = test['v2a1'].isnull()\n\ntest['v2a1-missing'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['rez_esc'].notnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['rez_esc'].isnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"Plot Two Categoricals\n\nWe draw a value count plot for where these values missing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['v18q1'] = train['v18q1'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)\ntrain['v2a1'] = train['v2a1'].fillna(0)\ntest['v2a1'] = test['v2a1'].fillna(0)\ntrain['rez_esc'] = train['rez_esc'].fillna(0)\ntest['rez_esc'] = test['rez_esc'].fillna(0)\ntrain['SQBmeaned'] = train['SQBmeaned'].fillna(0)\ntest['SQBmeaned'] = test['SQBmeaned'].fillna(0)\ntrain['meaneduc'] = train['meaneduc'].fillna(0)\ntest['meaneduc'] = test['meaneduc'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping unnecesary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['Id','idhogar','v2a1-missing'], inplace = True, axis =1)\n\ntest.drop(['Id','idhogar','v2a1-missing'], inplace = True, axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dividing the data into predictors & target"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.iloc[:,140]\ny.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.iloc[:,1:141]\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling  numeric features & applying PCA to reduce features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler as ss\n\nsm_imputer = SimpleImputer()\nX = sm_imputer.fit_transform(X)\nscale = ss()\nX = scale.fit_transform(X)\n#pca = PCA(0.95)\n#X = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into train & test "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Models\n\n### 1. ExtraTreeClassifier model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodeletf = ExtraTreesClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodeletf = modeletf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = modeletf.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1 = f1_score(y_test, classes, average='macro')\nf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tuning using Bayesian optimisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    ExtraTreesClassifier( ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {   'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeletfTuned=ExtraTreesClassifier(criterion=\"entropy\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodeletfTuned = modeletfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yetf=modeletfTuned.predict(X_test)\nyetf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yetftest=modeletfTuned.predict(test)\nyetftest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. GradientBoostingClassifier model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier as gbm\n\nmodelgbm=gbm()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelgbm = modelgbm.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = modelgbm.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = f1_score(y_test, classes, average='macro')\nf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tuning using Bayesian Optimization."},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    gbm(\n               # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        \n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 2                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelgbmTuned=gbm(\n               max_depth=84,\n               max_features=11,\n               min_weight_fraction_leaf=0.04840,\n               n_estimators=489)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelgbmTuned = modelgbmTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ygbm=modelgbmTuned.predict(X_test)\nygbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ygbmtest=modelgbmTuned.predict(test)\nygbmtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Light Gradient Booster model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nmodellgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodellgb = modellgb.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = modellgb.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = f1_score(y_test, classes, average='macro')\nf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### tuning using Bayesian Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    lgb.LGBMClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modellgbTuned = lgb.LGBMClassifier(criterion=\"gini\",\n               max_depth=5,\n               max_features=53,\n               min_weight_fraction_leaf=0.01674,\n               n_estimators=499)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodellgbTuned = modellgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ylgb=modellgbTuned.predict(X_test)\nylgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ylgbtest=modellgbTuned.predict(test)\nylgbtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Random Forest model"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrf = rf()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelrf = modelrf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = modelrf.predict(X_test)\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = f1_score(y_test, classes, average='macro')\nf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### tuning using Bayesian Optimisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    rf(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrfTuned=rf(criterion=\"entropy\",\n               max_depth=77,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelrfTuned = modelrfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yrf=modelrfTuned.predict(X_test)\nyrf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yrftest=modelrfTuned.predict(test)\nyrftest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                              ACCURACY           f1                ACCURACY\n                                with default parameters       score          with  parameters tuned with Bayesian                                                                                             Optimization \n        ExtraTreesClassifier           0.99                    0.99             1.0\n        GradientBoostingClassifier     1.0                     1.0              1.0 \n        LightGBM                       1.0                     1.0              1.0\n        RandomForestClassifier         1.0                     1.0              1.0"},{"metadata":{},"cell_type":"markdown","source":"## BUILDING NEW DATASETS with predicted values using 4 models"},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = pd.DataFrame()\n\ntrain1['yetf'] = yetf.tolist()\ntrain1['ygbm'] = ygbm.tolist()\ntrain1['ylgb'] = ylgb.tolist()\ntrain1['yrf'] = yrf.tolist()\n\ntrain1.head(5), train1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1 = pd.DataFrame()\n\ntest1['yetf'] = yetftest.tolist()\ntest1['ygbm'] = ygbmtest.tolist()\ntest1['ylgb'] = ylgbtest.tolist()\ntest1['yrf'] = yrftest.tolist()\n\ntest1.head(5), test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EnsembleModel=rf(criterion=\"entropy\",\n               max_depth=77,\n               max_features=4,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nEnsembleModel = EnsembleModel.fit(train1, y_test)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypredict=EnsembleModel.predict(test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypredict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ygbmtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit=pd.DataFrame({'Id': ids, 'Target': ygbmtest})\nsubmit.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submit.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}